{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fold 2 of 20210308 (epoch 8) is the model trained with almost all the data\n",
    "'''\n",
    "TRAIN_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/spgc/views/\"\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/spgc/views/\"\n",
    "\n",
    "SUB_FILE = ['axis1', 'axis2']\n",
    "# SUB_FILE = ['axis1']\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 448\n",
    "IMG_WIDTH = 448\n",
    "IMG_CHANNELS = 3\n",
    "NUM_CLASSES = 3\n",
    "DATA_FOLDER = 'spgc-ternario/'\n",
    "LOG_FOLDER = 'logs/' + DATA_FOLDER\n",
    "TRAINING_FOLDER = 'training/spgc-ternario/resnet101/fold/'\n",
    "MODEL_FOLDER = 'models/spgc-ternario/resnet101/fold/'\n",
    "COLUMN_TARGET = 'ternario'\n",
    "\n",
    "STRUCTURE_DATASET_FOLDER = \"csv/input/\" + DATA_FOLDER\n",
    "OUTPUT_PREDICTED_FOLDER = \"csv/output/\" + DATA_FOLDER\n",
    "OUTPUT_PREDICTED_FOLDER_TEST = OUTPUT_PREDICTED_FOLDER + \"/test\"\n",
    "\n",
    "TEST_FILE = f'csv/input/{DATA_FOLDER}/test/test2.csv'\n",
    "SAVE_TEST_RESULT = f'csv/output/{DATA_FOLDER}/test/test2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import utilities.plot_metrics as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(folder, search_filter=''):\n",
    "    '''\n",
    "    Get all files (full path) contained in a PATH folder by specified search filter \n",
    "    '''\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            if search_filter in path:\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def get_data_set(fold_number, cur_subfile, data_train, data_test):\n",
    "    ''' Creates and returns a dataframe with all the full paths (for slice) for train and test images. \n",
    "    Save it as log. \n",
    "    '''\n",
    "    dfs = []\n",
    "    train_images = {\"id\": [], \"label\": []}\n",
    "    validation_images = {\"id\": [], \"label\": []}\n",
    "    \n",
    "    \n",
    "    TRAIN_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_train.iterrows():\n",
    "        TRAIN_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "\n",
    "    VALIDATION_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_test.iterrows():\n",
    "        VALIDATION_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "    \n",
    "    df_config = [\n",
    "        (TRAIN_IMG_SRC_FOLDER, TRAIN_IMG_FOLDERS_SLICE, train_images),\n",
    "        (VALIDATION_IMG_SRC_FOLDER, VALIDATION_IMG_FOLDERS_SLICE, validation_images)\n",
    "    ]\n",
    "    for (base, folder, dic) in df_config:\n",
    "        for img_folder, img_label in folder.items():\n",
    "            search_folder = \"{}/{}\".format(base, img_folder)\n",
    "            imgs_filename = sorted(get_file_path(search_folder, search_filter = cur_subfile))\n",
    "            dic[\"id\"].extend(imgs_filename)\n",
    "            dic[\"label\"].extend([img_label] * len(imgs_filename))\n",
    "\n",
    "        dfs.append(pd.DataFrame(data=dic))\n",
    "#     print(dfs)\n",
    "    train_df, validation_df = dfs[0], dfs[1]\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"logs/\"): \n",
    "        os.mkdir(\"logs/\")\n",
    "    if not os.path.exists(LOG_FOLDER): \n",
    "        os.mkdir(LOG_FOLDER)\n",
    "        \n",
    "    train_df.to_csv(\"{}/train{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "    validation_df.to_csv(\"{}/test{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "\n",
    "    print(\"Train fold with {} images\".format(len(train_df)))\n",
    "    print(train_df.groupby(\"label\").label.count())\n",
    "    print()\n",
    "    print(\"Validation fold with {} images\".format(len(validation_df)))\n",
    "    print(validation_df.groupby(\"label\").label.count())\n",
    "    print(\"-\" * 30)\n",
    "    return (train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(dataframe, x_col, y_col, subset=None, shuffle=True, batch_size=32, class_mode=\"binary\"):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=False,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    )\n",
    "    \n",
    "    data_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        class_mode=class_mode,\n",
    "        # color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_by_patient(model, patients, legend_file, axis):\n",
    "    results = []\n",
    "    if os.path.isfile(legend_file+'.npy'):\n",
    "        class_indices = np.load(legend_file+'.npy', allow_pickle=True).item()\n",
    "        class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "    print(class_indices)\n",
    "    for p in patients:\n",
    "        # for axis in SUB_FILE:\n",
    "        curr_dir = \"{}/{}/{}\".format(VALIDATION_IMG_SRC_FOLDER, p, axis)\n",
    "        imgs_filename = sorted(os.listdir(curr_dir))\n",
    "        test_filenames = imgs_filename[:]\n",
    "        test_df = pd.DataFrame({\n",
    "                'filename': test_filenames\n",
    "            })\n",
    "        nb_samples = test_df.shape[0]\n",
    "\n",
    "        test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_gen.flow_from_dataframe(\n",
    "                test_df, \n",
    "                curr_dir, \n",
    "                x_col='filename',\n",
    "                y_col=None,\n",
    "                class_mode=None,\n",
    "                target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                batch_size=16,\n",
    "                shuffle=False\n",
    "        )\n",
    "\n",
    "        predict = model.predict(test_generator, steps=np.ceil(nb_samples/16))\n",
    "\n",
    "        test_df['predicted'] = [np.where(pr == np.max(pr))[0][0] for pr in predict]\n",
    "        for key in class_indices.keys():\n",
    "            t = class_indices[key]\n",
    "            test_df[t] = 0\n",
    "            test_df.loc[test_df['predicted'] == key, t] = 1\n",
    "        test_df['patient'] = p\n",
    "        test_df['axis'] = axis\n",
    "        results.append(test_df)\n",
    "\n",
    "    print('Axis: ', test_df['axis'][0])\n",
    "    df_result = pd.DataFrame()\n",
    "    for i,test_df in enumerate(results):\n",
    "        cur_patient = test_df['patient'][0]\n",
    "        test_df['predicted'] = 0\n",
    "        test_df['patient'] = 0\n",
    "        test_df = test_df.groupby(['patient', 'predicted'], as_index = False).sum()\n",
    "        test_df['predicted'] = test_df.idxmax(axis=1)\n",
    "        test_df['patient'] = cur_patient\n",
    "        df_result = df_result.append(test_df)\n",
    "        \n",
    "    return df_result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check models!\n",
    "Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "axis1\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "Fold \n",
      "{0: 'covid', 1: 'normal', 2: 'o_cap'}\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Found 82 validated image filenames.\n",
      "Axis:  axis1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csv/output/spgc-ternario//test/test2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4a7e1c849818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdf_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_TEST_RESULT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'csv/output/spgc-ternario//test/test2.csv'"
     ]
    }
   ],
   "source": [
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=======================================================================================')\n",
    "    \n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    \n",
    "    \n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame()\n",
    "    \n",
    "    for n_fold in ['']: \n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "        data_test = pd.read_csv(TEST_FILE)    \n",
    "        \n",
    "        legend_path = \"{}/{}/legend\".format(MODEL_FOLDER, axis)\n",
    "        model = tf.keras.models.load_model(\"{}/{}/my_checkpoint\".format(TRAINING_FOLDER, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_test['nome'].to_list(), legend_path, axis)\n",
    "        df_axis = df_axis.append(df)\n",
    "    \n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(SAVE_TEST_RESULT, index=False)\n",
    "    print(df_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
