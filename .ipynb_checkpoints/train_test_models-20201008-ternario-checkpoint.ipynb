{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TEST = \"exame-pulmao\"\n",
    "TRAIN_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf12-all/\" + FOLDER_TEST\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-vidro/\" + FOLDER_TEST\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HCPA-vidro/\" + FOLDER_TEST\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf12-all/\" + FOLDER_TEST\n",
    "\n",
    "# SUB_FILE = ['axis1', 'axis2', 'axis3', 'axis4']\n",
    "# SUB_FILE = ['axis1']\n",
    "SUB_FILE = ['axis1', 'axis2']\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 448\n",
    "IMG_WIDTH = 448\n",
    "IMG_CHANNELS = 3\n",
    "SELECTED_MODEL = ''\n",
    "NUM_CLASSES = 3\n",
    "# DATA_FOLDER = '20200827/'\n",
    "DATA_FOLDER = '20201008/'\n",
    "LOG_FOLDER = 'logs/' + DATA_FOLDER\n",
    "TRAINING_FOLDER = 'training/' + DATA_FOLDER\n",
    "MODEL_FOLDER = 'models/' + DATA_FOLDER\n",
    "IMAGE_FOLDER = 'images/' + DATA_FOLDER\n",
    "\n",
    "STRUCTURE_DATASET_FOLDER = \"csv/input/\"+DATA_FOLDER\n",
    "OUTPUT_PREDICTED_FOLDER = \"csv/output/\" + DATA_FOLDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import utilities.plot_metrics as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(folder, search_filter=''):\n",
    "    '''\n",
    "    Get all files (full path) contained in a PATH folder by specified search filter \n",
    "    '''\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            if search_filter in path:\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def get_data_set(fold_number, cur_subfile, data_train, data_test):\n",
    "    ''' Creates and returns a dataframe with all the full paths (for slice) for train and test images. \n",
    "    Save it as log. \n",
    "    '''\n",
    "    dfs = []\n",
    "    train_images = {\"id\": [], \"label\": []}\n",
    "    validation_images = {\"id\": [], \"label\": []}\n",
    "    \n",
    "    \n",
    "    TRAIN_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_train.iterrows():\n",
    "        TRAIN_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "\n",
    "    VALIDATION_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_test.iterrows():\n",
    "        VALIDATION_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "    \n",
    "    df_config = [\n",
    "        (TRAIN_IMG_SRC_FOLDER, TRAIN_IMG_FOLDERS_SLICE, train_images),\n",
    "        (VALIDATION_IMG_SRC_FOLDER, VALIDATION_IMG_FOLDERS_SLICE, validation_images)\n",
    "    ]\n",
    "    for (base, folder, dic) in df_config:\n",
    "        for img_folder, img_label in folder.items():\n",
    "            search_folder = \"{}/{}\".format(base, img_folder)\n",
    "            imgs_filename = sorted(get_file_path(search_folder, search_filter = cur_subfile))\n",
    "            dic[\"id\"].extend(imgs_filename)\n",
    "            dic[\"label\"].extend([img_label] * len(imgs_filename))\n",
    "\n",
    "        dfs.append(pd.DataFrame(data=dic))\n",
    "#     print(dfs)\n",
    "    train_df, validation_df = dfs[0], dfs[1]\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"logs/\"): \n",
    "        os.mkdir(\"logs/\")\n",
    "    if not os.path.exists(LOG_FOLDER): \n",
    "        os.mkdir(LOG_FOLDER)\n",
    "        \n",
    "    train_df.to_csv(\"{}/train{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "    validation_df.to_csv(\"{}/test{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "\n",
    "    print(\"Train fold with {} images\".format(len(train_df)))\n",
    "    print(train_df.groupby(\"label\").label.count())\n",
    "    print()\n",
    "    print(\"Validation fold with {} images\".format(len(validation_df)))\n",
    "    print(validation_df.groupby(\"label\").label.count())\n",
    "    print(\"-\" * 30)\n",
    "    return (train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side(img, side_type, side_size=5):\n",
    "    height, width, channel=img.shape\n",
    "    if side_type==\"horizontal\":\n",
    "        return np.ones((height,side_size,  channel), dtype=np.float32)*255\n",
    "        \n",
    "    return np.ones((side_size, width,  channel), dtype=np.float32)*255\n",
    "\n",
    "def show_gallery(show=\"all\"):\n",
    "    n=100\n",
    "    counter=0\n",
    "    images=list()\n",
    "    vertical_images=[]\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(train_images[\"id\"])\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(train_images[\"label\"])\n",
    "    for path, target in zip(train_images[\"id\"], train_images[\"label\"]):\n",
    "        if target!=show and show!=\"all\":\n",
    "            continue\n",
    "        counter=counter+1\n",
    "        if counter%100==0:\n",
    "            break\n",
    "        #Image loading from disk as JpegImageFile file format\n",
    "        img=load_img(path, target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
    "        #Converting JpegImageFile to numpy array\n",
    "        img=img_to_array(img)\n",
    "        \n",
    "        hside=get_side(img, side_type=\"horizontal\")\n",
    "        images.append(img)\n",
    "        images.append(hside)\n",
    "\n",
    "        if counter%10==0:\n",
    "            himage=np.hstack((images))\n",
    "            vside=get_side(himage, side_type=\"vertical\")\n",
    "            vertical_images.append(himage)\n",
    "            vertical_images.append(vside)\n",
    "            \n",
    "            images=list()\n",
    "\n",
    "    gallery=np.vstack((vertical_images)) \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    title = {\"all\":\"All Classifications's\",\n",
    "             \"healthy\":\"Healthy\",\n",
    "             \"covid\":\"Covid-19\"}\n",
    "    plt.title(\"100 Samples of {} Patients of the training set\".format(title[show]))\n",
    "    plt.imshow(gallery.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_gallery(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(dataframe, x_col, y_col, subset=None, shuffle=True, batch_size=32, class_mode=\"binary\"):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=False,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    )\n",
    "    \n",
    "    data_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        class_mode=class_mode,\n",
    "        # color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vgg16_chico():\n",
    "    with tf.device('/GPU:0'):\n",
    "#         conv_base = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        conv_base = get_base_model()\n",
    "\n",
    "        conv_base.trainable = True\n",
    "        set_trainable = False\n",
    "        for layer in conv_base.layers:\n",
    "            if layer.name == 'block1_conv1':\n",
    "                set_trainable = True\n",
    "            if set_trainable:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "        x = conv_base.output\n",
    "#         model = tf.keras.Sequential()\n",
    "#         model.add(conv_base)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#         model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        preds = tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'softmax')(x)\n",
    "#         model.add(tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'sigmoid'))\n",
    "\n",
    "        model = tf.keras.Model(inputs=conv_base.input, outputs=preds)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return (model, 'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vgg16():\n",
    "    with tf.device('/GPU:0'):\n",
    "        conv_base = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        conv_base.trainable = True\n",
    "        set_trainable = False\n",
    "        for layer in conv_base.layers:\n",
    "            if layer.name == 'block1_conv1':\n",
    "                set_trainable = True\n",
    "            if set_trainable:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(conv_base)\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        model.add(tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'sigmoid'))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return (model, 'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_resnet50():\n",
    "    with tf.device('/GPU:0'):\n",
    "        conv_base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        conv_base.trainable = True\n",
    "#         set_trainable = False\n",
    "#         for layer in conv_base.layers:\n",
    "#             if layer.name == 'block1_conv1':\n",
    "#                 set_trainable = True\n",
    "#             if set_trainable:\n",
    "#                 layer.trainable = True\n",
    "#             else:\n",
    "#                 layer.trainable = False\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(conv_base)\n",
    "        model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        model.add(tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'sigmoid'))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return (model, 'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_df, validation_df, epochs, fold, axis):\n",
    "    batch_size = 12\n",
    "    train_generator = get_data_generator(train_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "    validation_generator = get_data_generator(validation_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "\n",
    "    print(train_generator.class_indices)\n",
    "    print(validation_generator.class_indices)\n",
    "    \n",
    "    step_size_train = train_generator.n // train_generator.batch_size\n",
    "    step_size_validation = validation_generator.n // validation_generator.batch_size\n",
    "\n",
    "    if step_size_train == 0:\n",
    "        step_size_train = train_generator.n // 2\n",
    "        step_size_validation = validation_generator.n // 2\n",
    "        \n",
    "        \n",
    "    # callbacks, save each time\n",
    "    # training/20200827/vgg16/fold4/axis2\n",
    "    checkpoint_path = \"training/\"\n",
    "    if not os.path.exists(\"training/\"): \n",
    "        os.mkdir(\"training/\")\n",
    "    checkpoint_path = \"{}/\".format(TRAINING_FOLDER)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/fold{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "    \n",
    "    checkpoint_path = \"{}/{}/fold{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    # Save dict results of history and legend from current model\n",
    "    # models/20200827/vgg16/fold4/axis2/{history|legend}\n",
    "    if not os.path.exists(\"models/\"): \n",
    "        os.mkdir(\"models/\")\n",
    "    \n",
    "    model_dir = \"{}/\".format(MODEL_FOLDER)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}\".format(MODEL_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}/fold{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_dir = \"{}/{}/fold{}/{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "#     checkpoint_path = checkpoint_path + \"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_path = checkpoint_path +\"/my_checkpoint\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every 25 epochs\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=1,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "        \n",
    "    history = model.fit(train_generator, # X_Train\n",
    "        steps_per_epoch=step_size_train,\n",
    "        epochs=epochs, \n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=step_size_validation,\n",
    "        callbacks=cp_callback\n",
    "                       )\n",
    "    \n",
    "    # Save last values\n",
    "#     model.save_weights(checkpoint_dir+\"/my_checkpoint\")\n",
    "    # model.save(checkpoint_dir+\"/my_checkpoint\")\n",
    "    \n",
    "    # Save history\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = model_dir + 'history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    \n",
    "    # Save classes\n",
    "    print(train_generator.class_indices)\n",
    "    np.save(model_dir + 'legend', train_generator.class_indices)\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(history, sub_folder, fold, sel_model):\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    \n",
    "    image_dir = \"images/\"\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/\".format(IMAGE_FOLDER)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_dir + 'accuracy', pad_inches=0.1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(image_dir + 'loss', orientation='portrait', pad_inches=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate model for data folders (patients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_by_patient(model, patients, legend_file, axis):\n",
    "    results = []\n",
    "    for p in patients:\n",
    "        # for axis in SUB_FILE:\n",
    "        curr_dir = \"{}/{}/{}\".format(VALIDATION_IMG_SRC_FOLDER, p, axis)\n",
    "        imgs_filename = sorted(os.listdir(curr_dir))\n",
    "        test_filenames = imgs_filename[:]\n",
    "        test_df = pd.DataFrame({\n",
    "                'filename': test_filenames\n",
    "            })\n",
    "        nb_samples = test_df.shape[0]\n",
    "\n",
    "        test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_gen.flow_from_dataframe(\n",
    "                test_df, \n",
    "                curr_dir, \n",
    "                x_col='filename',\n",
    "                y_col=None,\n",
    "                class_mode=None,\n",
    "                target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                batch_size=16,\n",
    "                shuffle=False\n",
    "        )\n",
    "\n",
    "        predict = model.predict(test_generator, steps=np.ceil(nb_samples/16))\n",
    "\n",
    "        test_df['predicted'] = [np.where(pr == np.max(pr))[0][0] for pr in predict]\n",
    "        test_df['patient'] = p\n",
    "        test_df['axis'] = axis\n",
    "        results.append(test_df)\n",
    "\n",
    "    print('Axis: ', test_df['axis'][0])\n",
    "    df_result = pd.DataFrame(columns=['predicted', 'patient','count'])\n",
    "    for i,test_df in enumerate(results):\n",
    "        cur_patient = test_df['patient'][0]\n",
    "        if os.path.isfile(legend_file+'.npy'):\n",
    "            class_indices = np.load(legend_file+'.npy', allow_pickle=True).item()\n",
    "            class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "            test_df['predicted'] = test_df['predicted'].replace(class_indices)\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby('predicted', as_index = False)['count'].count()\n",
    "        test_df['patient'] = cur_patient\n",
    "        df_result = df_result.append(test_df)\n",
    "    return df_result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model for 'axis1...n' and folds [ 1...N ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Fold 1\n",
      "Train fold with 14186 images\n",
      "label\n",
      "INDETERMINATE    3936\n",
      "NEGATIVE         3034\n",
      "POSITIVE         7216\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Validation fold with 3444 images\n",
      "label\n",
      "INDETERMINATE     984\n",
      "NEGATIVE          738\n",
      "POSITIVE         1722\n",
      "Name: label, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "axis1\n",
      "=====\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 448, 448, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 448, 448, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 448, 448, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 224, 224, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 224, 224, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 112, 112, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 112, 112, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 112, 112, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 56, 56, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_10  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 14,716,227\n",
      "Trainable params: 14,716,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 14186 validated image filenames belonging to 3 classes.\n",
      "Found 3444 validated image filenames belonging to 3 classes.\n",
      "{'INDETERMINATE': 0, 'NEGATIVE': 1, 'POSITIVE': 2}\n",
      "{'INDETERMINATE': 0, 'NEGATIVE': 1, 'POSITIVE': 2}\n",
      " 288/1182 [======>.......................] - ETA: 20:24 - loss: 0.4386 - accuracy: 0.6687"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3586d02e0c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSELECTED_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_vgg16_chico\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#         model, SELECTED_MODEL = get_model_resnet50()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#Plot Results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d7136fa6f3fd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_df, validation_df, epochs, fold, axis)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                        )\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/noa/cidia19/jupyter/env-cnn/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    \n",
    "    ''' CREATE PATHS FOR SAVE OUTPUT '''\n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    ''''''\n",
    "    \n",
    "    ''' CREATE DATAFRAME AND CONFUSION MATRIX - ACCUMULATED '''\n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "    ''''''\n",
    "    \n",
    "#     for n_fold in [4]:\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_train = pd.read_csv(\"{}/train/train{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        train_df, validation_df = get_data_set(n_fold, axis, data_train, data_validation)\n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "        print('\\n'+axis+'\\n=====')\n",
    "#         model, SELECTED_MODEL = get_model_vgg16()\n",
    "        model, SELECTED_MODEL = get_model_vgg16_chico()\n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "        history = train_model(model, train_df, validation_df, EPOCHS, n_fold, axis)\n",
    "        \n",
    "        #Plot Results\n",
    "        plot_results(history, axis, n_fold, SELECTED_MODEL)\n",
    "        # Load legend\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        \n",
    "        '''  GET DATA  '''\n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        '''''' '''''' ''''''\n",
    "        \n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted.csv', index=False)\n",
    "    print(labels_pos_dict)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    '''\n",
    "    PRINT METRICS AND CONFUSION MATRIX\n",
    "    '''\n",
    "    pm.plot_labels_metrics(cm=confusion_matrix, normalize=False, labels=labels, show_zero=False,\n",
    "                title='Metrics', clear_diagonal=False, figsize=(15, 105), output_file=\"metrics.png\"\n",
    "                          )\n",
    "    pm.plot_confusion_matrix(cm=confusion_matrix, normalize=False, labels=labels, show_zero=False,\n",
    "                title=\"Confusion Matrix\", clear_diagonal=False, output_file=\"matrix.png\", figsize=(10, 7)\n",
    "                            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check models!\n",
    "Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HMV + HCPA\n",
    "'''\n",
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=======================================================================================')\n",
    "    \n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    \n",
    "    \n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "\n",
    "#     for n_fold in [1, 2, 3, 4, 5]: # [1, 2, 3, 4, 5]:\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "#     for n_fold in [j+31 for j in range(14)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        \n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "#         model, SELECTED_MODEL = get_model_vgg16()\n",
    "        model, SELECTED_MODEL = get_model_vgg16_chico()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        model = tf.keras.models.load_model(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        del model\n",
    "        \n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted.csv', index=False)\n",
    "    print(labels_pos_dict)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    '''\n",
    "    PRINT METRICS AND CONFUSION MATRIX\n",
    "    '''\n",
    "    pm.plot_labels_metrics(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title='Metrics - ' + axis,\n",
    "                clear_diagonal=False,\n",
    "                figsize=(15, 105),\n",
    "                output_file=\"metrics.png\"\n",
    "            )\n",
    "    pm.plot_confusion_matrix(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title=\"Confusion Matrix - \" + axis,\n",
    "                clear_diagonal=False,\n",
    "                output_file=\"matrix.png\",\n",
    "                figsize=(10, 7)\n",
    "#                 verbose=args.verbose,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, SELECTED_MODEL = get_model_resnet50()\n",
    "model, SELECTED_MODEL = get_model_vgg16()\n",
    "\n",
    "# model.load_weights('training/resnet50/5fold/axis1/my_checkpoint')\n",
    "fold = 1\n",
    "axis = 'axis1'\n",
    "legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "model.load_weights(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, fold, axis))\n",
    "\n",
    "# data_test =  ['C77', 'C117', 'C136', 'C80', 'C91', 'C104', 'C62', 'C147', 'C106', 'C68', 'C123', 'C99', 'C129']\n",
    "# HCPA\n",
    "# data_test = ['NEG-001', 'NEG-002', 'NEG-003', 'NEG-004', 'NEG-005', 'NEG-006', 'NEG-007', 'NEG-008', 'NEG-009', \n",
    "#              'NEG-010', 'NEG-011', 'NEG-012', 'NEG-013', 'NEG-014', 'NEG-015', 'TYP-002', 'TYP-003', 'TYP-004', \n",
    "#              'TYP-005', 'TYP-006', 'TYP-007', 'TYP-008', 'TYP-009', 'TYP-010', 'TYP-011', 'TYP-012', 'TYP-013', \n",
    "#              'TYP-014', 'TYP-015', 'TYP-016', 'TYP-017', 'TYP-018', 'TYP-019', 'TYP-020', 'TYP-021', 'TYP-022', \n",
    "#              'TYP-023', 'TYP-024', 'TYP-025', 'TYP-026', 'TYP-027', 'TYP-028', 'TYP-029', 'TYP-030', 'TYP-031']\n",
    "\n",
    "# HMV - CT Indeterminados - PCR Negativo\n",
    "# data_test = ['C8', 'C28', 'C30', 'C31', 'C34', 'C37', 'C38', 'C45', 'C47', 'C54', 'C68', 'C72', 'C84', 'C98', \n",
    "#              'C99', 'C109', 'C119', 'C123', 'C129', 'C139', 'C140', 'C148', 'C156']\n",
    "\n",
    "# HMV - CT Indeterminados - PCR Positivo\n",
    "# data_test = ['C40', 'C48', 'C57', 'C65', 'C97', 'C107', 'C128']\n",
    "\n",
    "# HMV - Atípicos\n",
    "# data_test = ['C9', 'C43', 'C55', 'C56', 'C58', 'C59', 'C64', 'C67', 'C70', 'C73', 'C81', 'C118', 'C122', 'C127', \n",
    "#              'C134', 'C141', 'C164']\n",
    "\n",
    "# HMV - CT Típico - PCR negativo\n",
    "# data_test = ['C71', 'C101', 'C143', 'C162'] \n",
    "\n",
    "# HMV - CT Negativo - PCR positivo\n",
    "data_test = ['C76', 'C105']\n",
    "\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HCPA/exame-pulmao\"\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV/exame-pulmao\"\n",
    "\n",
    "df = predictions_by_patient(model, data_test, legend_path, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HCPA\n",
    "'''\n",
    "data_test = ['NEG-001', 'NEG-002', 'NEG-003', 'NEG-004', 'NEG-005', 'NEG-006', 'NEG-007', 'NEG-008', 'NEG-009', \n",
    "             'NEG-010', 'NEG-011', 'NEG-012', 'NEG-013', 'NEG-014', 'NEG-015', 'TYP-002', 'TYP-003', 'TYP-004', \n",
    "             'TYP-005', 'TYP-006', 'TYP-007', 'TYP-008', 'TYP-009', 'TYP-010', 'TYP-011', 'TYP-012', 'TYP-013', \n",
    "             'TYP-014', 'TYP-015', 'TYP-016', 'TYP-017', 'TYP-018', 'TYP-019', 'TYP-020', 'TYP-021', 'TYP-022', \n",
    "             'TYP-023', 'TYP-024', 'TYP-025', 'TYP-026', 'TYP-027', 'TYP-028', 'TYP-029', 'TYP-030', 'TYP-031']\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=====')\n",
    "    for n_fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "        model, SELECTED_MODEL = get_model_vgg16()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        model.load_weights(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_test, legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        print(df[idx][['category', 'patient', 'count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=====')\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "        #model, SELECTED_MODEL = get_model_resnet50()\n",
    "        SELECTED_MODEL = 'vgg16'\n",
    "        df = pd.read_csv(\"{}/{}/fold{}/{}/history.csv\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        print(max(df['val_accuracy']))\n",
    "        print((df['val_accuracy'].mean()))\n",
    "#         print(df['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
