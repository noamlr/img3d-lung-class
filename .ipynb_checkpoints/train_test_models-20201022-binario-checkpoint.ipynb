{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TEST = \"exame-pulmao\"\n",
    "TRAIN_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf22-all/\" + FOLDER_TEST\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-vidro/\" + FOLDER_TEST\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HCPA-vidro/\" + FOLDER_TEST\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf22-all/\" + FOLDER_TEST\n",
    "\n",
    "# SUB_FILE = ['axis1', 'axis2', 'axis3', 'axis4', 'axis5', 'axis6']\n",
    "SUB_FILE = ['axis1']\n",
    "# SUB_FILE = ['axis3', 'axis4']\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 256 # 448\n",
    "IMG_WIDTH = 256 # 448\n",
    "IMG_CHANNELS = 3\n",
    "SELECTED_MODEL = ''\n",
    "NUM_CLASSES = 2\n",
    "DATA_FOLDER = '20201022/'\n",
    "LOG_FOLDER = 'logs/' + DATA_FOLDER\n",
    "TRAINING_FOLDER = 'training/' + DATA_FOLDER\n",
    "MODEL_FOLDER = 'models/' + DATA_FOLDER\n",
    "IMAGE_FOLDER = 'images/' + DATA_FOLDER\n",
    "\n",
    "STRUCTURE_DATASET_FOLDER = \"csv/input/\"+DATA_FOLDER\n",
    "OUTPUT_PREDICTED_FOLDER = \"csv/output/\" + DATA_FOLDER\n",
    "LABELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import utilities.plot_metrics as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(folder, search_filter=''):\n",
    "    '''\n",
    "    Get all files (full path) contained in a PATH folder by specified search filter \n",
    "    '''\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            if search_filter in path:\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def get_data_set(fold_number, cur_subfile, data_train, data_test):\n",
    "    ''' Creates and returns a dataframe with all the full paths (for slice) for train and test images. \n",
    "    Save it as log. \n",
    "    '''\n",
    "    dfs = []\n",
    "    train_images = {\"id1\": [], \"id2\": [], \"id3\": [], \"id4\": [], \"id5\": [], \"id6\": [], \"label\": []}\n",
    "    validation_images = {\"id1\": [], \"id2\": [], \"id3\": [], \"id4\": [], \"id5\": [], \"id6\": [], \"label\": []}\n",
    "    \n",
    "    \n",
    "    TRAIN_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_train.iterrows():\n",
    "        TRAIN_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "\n",
    "    VALIDATION_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_test.iterrows():\n",
    "        VALIDATION_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "    \n",
    "    df_config = [\n",
    "        (TRAIN_IMG_SRC_FOLDER, TRAIN_IMG_FOLDERS_SLICE, train_images),\n",
    "        (VALIDATION_IMG_SRC_FOLDER, VALIDATION_IMG_FOLDERS_SLICE, validation_images)\n",
    "    ]\n",
    "    for (base, folder, dic) in df_config:\n",
    "        for img_folder, img_label in folder.items():\n",
    "            search_folder = \"{}/{}\".format(base, img_folder)\n",
    "            imgs_filename = sorted(get_file_path(search_folder, search_filter = cur_subfile))\n",
    "            dic[\"id1\"].extend(imgs_filename)\n",
    "            dic[\"id2\"].extend([i.replace('axis1', 'axis2').replace('3D_View1', '3D_View2') for i in imgs_filename])\n",
    "            dic[\"id3\"].extend([i.replace('axis1', 'axis3').replace('3D_View1', '3D_View3') for i in imgs_filename])\n",
    "            dic[\"id4\"].extend([i.replace('axis1', 'axis4').replace('3D_View1', '3D_View4') for i in imgs_filename])\n",
    "            dic[\"id5\"].extend([i.replace('axis1', 'axis5').replace('3D_View1', '3D_View5') for i in imgs_filename])\n",
    "            dic[\"id6\"].extend([i.replace('axis1', 'axis6').replace('3D_View1', '3D_View6') for i in imgs_filename])\n",
    "            dic[\"label\"].extend([img_label] * len(imgs_filename))\n",
    "\n",
    "        dfs.append(pd.DataFrame(data=dic))\n",
    "#     print(dfs)\n",
    "    train_df, validation_df = dfs[0], dfs[1]\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"logs/\"): \n",
    "        os.mkdir(\"logs/\")\n",
    "    if not os.path.exists(LOG_FOLDER): \n",
    "        os.mkdir(LOG_FOLDER)\n",
    "        \n",
    "    train_df.to_csv(\"{}/train{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "    validation_df.to_csv(\"{}/test{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "\n",
    "    print(\"Train fold with {} images\".format(len(train_df)))\n",
    "    print(train_df.groupby(\"label\").label.count())\n",
    "    print()\n",
    "    print(\"Validation fold with {} images\".format(len(validation_df)))\n",
    "    print(validation_df.groupby(\"label\").label.count())\n",
    "    print(\"-\" * 30)\n",
    "    return (train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side(img, side_type, side_size=5):\n",
    "    height, width, channel=img.shape\n",
    "    if side_type==\"horizontal\":\n",
    "        return np.ones((height,side_size,  channel), dtype=np.float32)*255\n",
    "        \n",
    "    return np.ones((side_size, width,  channel), dtype=np.float32)*255\n",
    "\n",
    "def show_gallery(show=\"all\"):\n",
    "    n=100\n",
    "    counter=0\n",
    "    images=list()\n",
    "    vertical_images=[]\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(train_images[\"id\"])\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(train_images[\"label\"])\n",
    "    for path, target in zip(train_images[\"id\"], train_images[\"label\"]):\n",
    "        if target!=show and show!=\"all\":\n",
    "            continue\n",
    "        counter=counter+1\n",
    "        if counter%100==0:\n",
    "            break\n",
    "        #Image loading from disk as JpegImageFile file format\n",
    "        img=load_img(path, target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
    "        #Converting JpegImageFile to numpy array\n",
    "        img=img_to_array(img)\n",
    "        \n",
    "        hside=get_side(img, side_type=\"horizontal\")\n",
    "        images.append(img)\n",
    "        images.append(hside)\n",
    "\n",
    "        if counter%10==0:\n",
    "            himage=np.hstack((images))\n",
    "            vside=get_side(himage, side_type=\"vertical\")\n",
    "            vertical_images.append(himage)\n",
    "            vertical_images.append(vside)\n",
    "            \n",
    "            images=list()\n",
    "\n",
    "    gallery=np.vstack((vertical_images)) \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    title = {\"all\":\"All Classifications's\",\n",
    "             \"healthy\":\"Healthy\",\n",
    "             \"covid\":\"Covid-19\"}\n",
    "    plt.title(\"100 Samples of {} Patients of the training set\".format(title[show]))\n",
    "    plt.imshow(gallery.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_gallery(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(dataframe, x_col, y_col, subset=None, shuffle=True, batch_size=32, class_mode=\"binary\"):\n",
    "    global LABELS\n",
    "#     print(f'y_col: {y_col}, mode:{class_mode}')\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=False,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    )\n",
    "    \n",
    "    data_generator1 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'1', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "        # color_mode=\"rgb\",\n",
    "    )\n",
    "    \n",
    "    data_generator2 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'2', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    data_generator3 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'3', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    data_generator4 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'4', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    data_generator5 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'5', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    data_generator6 = datagen.flow_from_dataframe(dataframe=dataframe, x_col=x_col+'6', y_col=y_col, subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT), class_mode=class_mode, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    LABELS = data_generator1.class_indices\n",
    "#     print(LABELS, data_generator2.class_indices, data_generator3.class_indices, data_generator4.class_indices,\n",
    "#          data_generator5.class_indices, data_generator6.class_indices)\n",
    "    if LABELS != data_generator2.class_indices or LABELS != data_generator3.class_indices:\n",
    "        print(\"\\n\\nLABELS 1\\n\\n\")\n",
    "    if LABELS != data_generator4.class_indices or LABELS != data_generator4.class_indices:\n",
    "        print(\"\\n\\nLABELS 2\\n\\n\")\n",
    "    if LABELS != data_generator6.class_indices:\n",
    "        print(\"\\n\\nLABELS 3\\n\\n\")\n",
    "    \n",
    "    while True:\n",
    "        x_1 = data_generator1.next()\n",
    "        x_2 = data_generator2.next()\n",
    "        x_3 = data_generator3.next()\n",
    "        x_4 = data_generator4.next()\n",
    "        x_5 = data_generator5.next()\n",
    "        x_6 = data_generator6.next()\n",
    "\n",
    "        yield [x_1[0], x_2[0], x_3[0], x_4[0], x_5[0], x_6[0]], x_1[1]\n",
    "    \n",
    "#     return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(id_label):\n",
    "    base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer._name = layer._name + id_label\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in base_model.layers:\n",
    "        if layer._name == 'block1_conv1' + id_label:\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vgg16_chico():\n",
    "    with tf.device('/GPU:0'):\n",
    "#         conv_base = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "#         conv_base = get_base_model()\n",
    "        conv_base_a1 = get_base_model('_a1')\n",
    "        conv_base_a2 = get_base_model('_a2')\n",
    "        conv_base_a3 = get_base_model('_a3')\n",
    "        conv_base_a4 = get_base_model('_a4')\n",
    "        conv_base_a5 = get_base_model('_a5')\n",
    "        conv_base_a6 = get_base_model('_a6')\n",
    "        \n",
    "#         x = conv_base_a1.output\n",
    "        \n",
    "        \n",
    "#         conv_base_a1 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a1.output)\n",
    "#         conv_base_a1 = tf.keras.layers.Flatten()(conv_base_a1)\n",
    "        \n",
    "#         conv_base_a2 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a2.output)\n",
    "#         conv_base_a2 = tf.keras.layers.Flatten()(conv_base_a2)\n",
    "        \n",
    "#         conv_base_a3 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a3.output)\n",
    "#         conv_base_a3 = tf.keras.layers.Flatten()(conv_base_a3)\n",
    "        \n",
    "#         conv_base_a4 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a4.output)\n",
    "#         conv_base_a4 = tf.keras.layers.Flatten()(conv_base_a4)\n",
    "        \n",
    "#         conv_base_a5 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a5.output)\n",
    "#         conv_base_a5 = tf.keras.layers.Flatten()(conv_base_a5)\n",
    "        \n",
    "#         conv_base_a6 = tf.keras.layers.GlobalAveragePooling2D()(conv_base_a6.output)\n",
    "#         conv_base_a6 = tf.keras.layers.Flatten()(conv_base_a6)\n",
    "        \n",
    "        \n",
    "\n",
    "#         merged_model = tf.keras.layers.Concatenate()([conv_base_a1, conv_base_a2, conv_base_a3, conv_base_a4,\n",
    "#                                                conv_base_a5, conv_base_a6])\n",
    "        merged_model = tf.keras.layers.Concatenate()([conv_base_a1.output, conv_base_a2.output,\n",
    "                                               conv_base_a3.output, conv_base_a4.output,\n",
    "                                               conv_base_a5.output, conv_base_a6.output])\n",
    "        \n",
    "        merged_model = tf.keras.layers.GlobalAveragePooling2D()(merged_model)\n",
    "        merged_model = tf.keras.layers.Dense(units = 1024)(merged_model)\n",
    "        merged_model = tf.keras.layers.Dense(units = 1024)(merged_model)\n",
    "#         merged_model = tf.keras.layers.Dense(units = NUM_CLASSES,activation = 'softmax')(merged_model)\n",
    "        \n",
    "#         x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        preds = tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'softmax')(merged_model)\n",
    "#         model.add(tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'sigmoid'))\n",
    "\n",
    "#         model = tf.keras.Model(inputs=[conv_base_a1, conv_base_a2, conv_base_a3, \n",
    "#                                        conv_base_a4, conv_base_a5, conv_base_a6], \n",
    "#                                    outputs=preds)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[conv_base_a1.input, conv_base_a2.input, conv_base_a3.input, \n",
    "                                       conv_base_a4.input, conv_base_a5.input, conv_base_a6.input], \n",
    "                               outputs=preds)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return (model, 'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_df, validation_df, epochs, fold, axis):\n",
    "    batch_size = 6\n",
    "    train_generator = get_data_generator(train_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "    validation_generator = get_data_generator(validation_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "\n",
    "#     print(train_generator.class_indices)\n",
    "#     print(validation_generator.class_indices)\n",
    "    print(validation_generator)\n",
    "\n",
    "#     step_size_train = train_generator.n // train_generator.batch_size\n",
    "#     step_size_validation = validation_generator.n // validation_generator.batch_size\n",
    "    \n",
    "    step_size_train = len(train_df) // batch_size\n",
    "    step_size_validation = len(validation_df) // batch_size\n",
    "\n",
    "    if step_size_train == 0:\n",
    "        step_size_train = len(train_df) // 2\n",
    "        step_size_validation = len(validation_df) // 2\n",
    "        \n",
    "        \n",
    "    # callbacks, save each time\n",
    "    # training/20200827/vgg16/fold4/axis2\n",
    "    checkpoint_path = \"training/\"\n",
    "    if not os.path.exists(\"training/\"): \n",
    "        os.mkdir(\"training/\")\n",
    "    checkpoint_path = \"{}/\".format(TRAINING_FOLDER)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/fold{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "    \n",
    "    checkpoint_path = \"{}/{}/fold{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    # Save dict results of history and legend from current model\n",
    "    # models/20200827/vgg16/fold4/axis2/{history|legend}\n",
    "    if not os.path.exists(\"models/\"): \n",
    "        os.mkdir(\"models/\")\n",
    "    \n",
    "    model_dir = \"{}/\".format(MODEL_FOLDER)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}\".format(MODEL_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}/fold{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_dir = \"{}/{}/fold{}/{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "#     checkpoint_path = checkpoint_path + \"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_path = checkpoint_path +\"/my_checkpoint\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every 25 epochs\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=1,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "        \n",
    "    history = model.fit(train_generator, # X_Train\n",
    "        steps_per_epoch=step_size_train,\n",
    "        epochs=epochs, \n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=step_size_validation,\n",
    "        callbacks=cp_callback\n",
    "                       )\n",
    "    \n",
    "    # Save last values\n",
    "#     model.save_weights(checkpoint_dir+\"/my_checkpoint\")\n",
    "    # model.save(checkpoint_dir+\"/my_checkpoint\")\n",
    "    \n",
    "    # Save history\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = model_dir + 'history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    \n",
    "    # Save classes\n",
    "#     print(train_generator.class_indices)\n",
    "    print(LABELS)\n",
    "#     np.save(model_dir + 'legend', train_generator.class_indices)\n",
    "    np.save(model_dir + 'legend', LABELS)\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(history, sub_folder, fold, sel_model):\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    \n",
    "    image_dir = \"images/\"\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/\".format(IMAGE_FOLDER)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_dir + 'accuracy', pad_inches=0.1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(image_dir + 'loss', orientation='portrait', pad_inches=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate model for data folders (patients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_by_patient(model, patients, legend_file, axis):\n",
    "    \n",
    "    print('Axis: ', axis)\n",
    "    print(legend_file+'.npy')\n",
    "    print(np.load(legend_file+'.npy', allow_pickle=True).item())\n",
    "    \n",
    "    results = []\n",
    "    for p in patients:\n",
    "        # for axis in SUB_FILE:\n",
    "        curr_dir = \"{}/{}/{}\".format(VALIDATION_IMG_SRC_FOLDER, p, axis)\n",
    "        imgs_filename = sorted(os.listdir(curr_dir))\n",
    "        test_filenames = imgs_filename[:]\n",
    "        test_filenames = [curr_dir + '/' + i for i in test_filenames]\n",
    "        test_df = pd.DataFrame({\n",
    "                'id1': test_filenames,\n",
    "                'id2': [i.replace('axis1', 'axis2').replace('3D_View1', '3D_View2') for i in test_filenames],\n",
    "                'id3': [i.replace('axis1', 'axis3').replace('3D_View1', '3D_View3') for i in test_filenames],\n",
    "                'id4': [i.replace('axis1', 'axis4').replace('3D_View1', '3D_View4') for i in test_filenames],\n",
    "                'id5': [i.replace('axis1', 'axis5').replace('3D_View1', '3D_View5') for i in test_filenames],\n",
    "                'id6': [i.replace('axis1', 'axis6').replace('3D_View1', '3D_View6') for i in test_filenames],\n",
    "                'label': [str(i%NUM_CLASSES) for i in range(len(test_filenames))]\n",
    "            })\n",
    "#         print(test_df)\n",
    "        nb_samples = test_df.shape[0]\n",
    "\n",
    "#         test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "#         test_generator = test_gen.flow_from_dataframe(\n",
    "#                 test_df, \n",
    "#                 curr_dir, \n",
    "#                 x_col='filename',\n",
    "#                 y_col=None,\n",
    "#                 class_mode=None,\n",
    "#                 target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "#                 batch_size=16,\n",
    "#                 shuffle=False\n",
    "#         )\n",
    "        test_generator = get_data_generator(test_df, \"id\", \"label\", batch_size=16)\n",
    "        predict = model.predict(test_generator, steps=np.ceil(nb_samples/16))\n",
    "\n",
    "        test_df['predicted'] = [np.where(pr == np.max(pr))[0][0] for pr in predict]\n",
    "        test_df['patient'] = p\n",
    "        test_df['axis'] = axis\n",
    "        results.append(test_df)\n",
    "\n",
    "    df_result = pd.DataFrame(columns=['predicted', 'patient','count'])\n",
    "    for i,test_df in enumerate(results):\n",
    "        cur_patient = test_df['patient'][0]\n",
    "        if os.path.isfile(legend_file+'.npy'):\n",
    "            class_indices = np.load(legend_file+'.npy', allow_pickle=True).item()\n",
    "            class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "            test_df['predicted'] = test_df['predicted'].replace(class_indices)\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby('predicted', as_index = False)['count'].count()\n",
    "        test_df['patient'] = cur_patient\n",
    "        df_result = df_result.append(test_df)\n",
    "    return df_result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model for 'axis1...n' and folds [ 1...N ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    \n",
    "    ''' CREATE PATHS FOR SAVE OUTPUT '''\n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    ''''''\n",
    "    \n",
    "    ''' CREATE DATAFRAME AND CONFUSION MATRIX - ACCUMULATED '''\n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "    ''''''\n",
    "    \n",
    "#     for n_fold in [1]:\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_train = pd.read_csv(\"{}/train/train{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        train_df, validation_df = get_data_set(n_fold, axis, data_train, data_validation)\n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "        print('\\n'+axis+'\\n=====')\n",
    "#         model, SELECTED_MODEL = get_model_vgg16()\n",
    "        model, SELECTED_MODEL = get_model_vgg16_chico()\n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "        history = train_model(model, train_df, validation_df, EPOCHS, n_fold, axis)\n",
    "        \n",
    "        #Plot Results\n",
    "        plot_results(history, axis, n_fold, SELECTED_MODEL)\n",
    "        # Load legend\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        \n",
    "        '''  GET DATA  '''\n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        '''''' '''''' ''''''\n",
    "        \n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted.csv', index=False)\n",
    "    print(labels_pos_dict)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    '''\n",
    "    PRINT METRICS AND CONFUSION MATRIX\n",
    "    '''\n",
    "    pm.plot_labels_metrics(cm=confusion_matrix, normalize=False, labels=labels, show_zero=False,\n",
    "                title='Metrics', clear_diagonal=False, figsize=(15, 105), output_file=\"metrics.png\"\n",
    "                          )\n",
    "    pm.plot_confusion_matrix(cm=confusion_matrix, normalize=False, labels=labels, show_zero=False,\n",
    "                title=\"Confusion Matrix\", clear_diagonal=False, output_file=\"matrix.png\", figsize=(10, 7)\n",
    "                            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check models!\n",
    "Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HMV + HCPA\n",
    "'''\n",
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=======================================================================================')\n",
    "    \n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    \n",
    "    \n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "\n",
    "#     for n_fold in [1]: # [1, 2, 3, 4, 5]:\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "#     for n_fold in [j+31 for j in range(14)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        \n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "#         model, SELECTED_MODEL = get_model_vgg16()\n",
    "        model, SELECTED_MODEL = get_model_vgg16_chico()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        model = tf.keras.models.load_model(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "#         print(labels_pos_dict)\n",
    "#         print(data_validation_dict)\n",
    "        for index, row in df.iterrows():\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        del model\n",
    "    print(df_axis)\n",
    "        \n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted.csv', index=False)\n",
    "    print(labels_pos_dict)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    '''\n",
    "    PRINT METRICS AND CONFUSION MATRIX\n",
    "    '''\n",
    "    pm.plot_labels_metrics(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title='Metrics - ' + axis,\n",
    "                clear_diagonal=False,\n",
    "                figsize=(15, 105),\n",
    "                output_file=\"metrics.png\"\n",
    "            )\n",
    "    pm.plot_confusion_matrix(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title=\"Confusion Matrix - \" + axis,\n",
    "                clear_diagonal=False,\n",
    "                output_file=\"matrix.png\",\n",
    "                figsize=(10, 7)\n",
    "#                 verbose=args.verbose,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, SELECTED_MODEL = get_model_resnet50()\n",
    "model, SELECTED_MODEL = get_model_vgg16()\n",
    "\n",
    "# model.load_weights('training/resnet50/5fold/axis1/my_checkpoint')\n",
    "fold = 1\n",
    "axis = 'axis1'\n",
    "legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "model.load_weights(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, fold, axis))\n",
    "\n",
    "# data_test =  ['C77', 'C117', 'C136', 'C80', 'C91', 'C104', 'C62', 'C147', 'C106', 'C68', 'C123', 'C99', 'C129']\n",
    "# HCPA\n",
    "# data_test = ['NEG-001', 'NEG-002', 'NEG-003', 'NEG-004', 'NEG-005', 'NEG-006', 'NEG-007', 'NEG-008', 'NEG-009', \n",
    "#              'NEG-010', 'NEG-011', 'NEG-012', 'NEG-013', 'NEG-014', 'NEG-015', 'TYP-002', 'TYP-003', 'TYP-004', \n",
    "#              'TYP-005', 'TYP-006', 'TYP-007', 'TYP-008', 'TYP-009', 'TYP-010', 'TYP-011', 'TYP-012', 'TYP-013', \n",
    "#              'TYP-014', 'TYP-015', 'TYP-016', 'TYP-017', 'TYP-018', 'TYP-019', 'TYP-020', 'TYP-021', 'TYP-022', \n",
    "#              'TYP-023', 'TYP-024', 'TYP-025', 'TYP-026', 'TYP-027', 'TYP-028', 'TYP-029', 'TYP-030', 'TYP-031']\n",
    "\n",
    "# HMV - CT Indeterminados - PCR Negativo\n",
    "# data_test = ['C8', 'C28', 'C30', 'C31', 'C34', 'C37', 'C38', 'C45', 'C47', 'C54', 'C68', 'C72', 'C84', 'C98', \n",
    "#              'C99', 'C109', 'C119', 'C123', 'C129', 'C139', 'C140', 'C148', 'C156']\n",
    "\n",
    "# HMV - CT Indeterminados - PCR Positivo\n",
    "# data_test = ['C40', 'C48', 'C57', 'C65', 'C97', 'C107', 'C128']\n",
    "\n",
    "# HMV - Atípicos\n",
    "# data_test = ['C9', 'C43', 'C55', 'C56', 'C58', 'C59', 'C64', 'C67', 'C70', 'C73', 'C81', 'C118', 'C122', 'C127', \n",
    "#              'C134', 'C141', 'C164']\n",
    "\n",
    "# HMV - CT Típico - PCR negativo\n",
    "# data_test = ['C71', 'C101', 'C143', 'C162'] \n",
    "\n",
    "# HMV - CT Negativo - PCR positivo\n",
    "data_test = ['C76', 'C105']\n",
    "\n",
    "# VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HCPA/exame-pulmao\"\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV/exame-pulmao\"\n",
    "\n",
    "df = predictions_by_patient(model, data_test, legend_path, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HCPA\n",
    "'''\n",
    "data_test = ['NEG-001', 'NEG-002', 'NEG-003', 'NEG-004', 'NEG-005', 'NEG-006', 'NEG-007', 'NEG-008', 'NEG-009', \n",
    "             'NEG-010', 'NEG-011', 'NEG-012', 'NEG-013', 'NEG-014', 'NEG-015', 'TYP-002', 'TYP-003', 'TYP-004', \n",
    "             'TYP-005', 'TYP-006', 'TYP-007', 'TYP-008', 'TYP-009', 'TYP-010', 'TYP-011', 'TYP-012', 'TYP-013', \n",
    "             'TYP-014', 'TYP-015', 'TYP-016', 'TYP-017', 'TYP-018', 'TYP-019', 'TYP-020', 'TYP-021', 'TYP-022', \n",
    "             'TYP-023', 'TYP-024', 'TYP-025', 'TYP-026', 'TYP-027', 'TYP-028', 'TYP-029', 'TYP-030', 'TYP-031']\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=====')\n",
    "    for n_fold in [1, 2, 3, 4, 5]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "#         model, SELECTED_MODEL = get_model_resnet50()\n",
    "        model, SELECTED_MODEL = get_model_vgg16()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        model.load_weights(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_test, legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        print(df[idx][['category', 'patient', 'count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=====')\n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "        #model, SELECTED_MODEL = get_model_resnet50()\n",
    "        SELECTED_MODEL = 'vgg16'\n",
    "        df = pd.read_csv(\"{}/{}/fold{}/{}/history.csv\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        print(max(df['val_accuracy']))\n",
    "        print((df['val_accuracy'].mean()))\n",
    "#         print(df['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
