{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TEST = \"exame-pulmao\"\n",
    "TRAIN_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf12-all/\" + FOLDER_TEST\n",
    "VALIDATION_IMG_SRC_FOLDER = \"/home/guilherme/Documents/noa/cidia19/data/output-2d/HMV-HCPA-tf12-all/\" + FOLDER_TEST\n",
    "\n",
    "SUB_FILE = ['axis1', 'axis2']\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 32# 448\n",
    "IMG_WIDTH = 32 # 448\n",
    "IMG_CHANNELS = 3\n",
    "SELECTED_MODEL = ''\n",
    "NUM_CLASSES = 2\n",
    "# DATA_FOLDER = '20201217-2/'\n",
    "DATA_FOLDER = '20210308/'\n",
    "LOG_FOLDER = 'logs/' + DATA_FOLDER\n",
    "TRAINING_FOLDER = 'training/' + DATA_FOLDER\n",
    "MODEL_FOLDER = 'models/' + DATA_FOLDER\n",
    "IMAGE_FOLDER = 'images/' + DATA_FOLDER\n",
    "\n",
    "STRUCTURE_DATASET_FOLDER = \"csv/input/\"+DATA_FOLDER\n",
    "OUTPUT_PREDICTED_FOLDER = \"csv/output/\" + DATA_FOLDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import utilities.plot_metrics as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(folder, search_filter=''):\n",
    "    '''\n",
    "    Get all files (full path) contained in a PATH folder by specified search filter \n",
    "    '''\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            if search_filter in path:\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def get_data_set(fold_number, cur_subfile, data_train, data_test):\n",
    "    ''' Creates and returns a dataframe with all the full paths (for slice) for train and test images. \n",
    "    Save it as log. \n",
    "    '''\n",
    "    dfs = []\n",
    "    train_images = {\"id\": [], \"label\": []}\n",
    "    validation_images = {\"id\": [], \"label\": []}\n",
    "    \n",
    "    \n",
    "    TRAIN_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_train.iterrows():\n",
    "        TRAIN_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "\n",
    "    VALIDATION_IMG_FOLDERS_SLICE = {}\n",
    "    for _, row in data_test.iterrows():\n",
    "        VALIDATION_IMG_FOLDERS_SLICE[row['nome']] = row['covid']\n",
    "    \n",
    "    df_config = [\n",
    "        (TRAIN_IMG_SRC_FOLDER, TRAIN_IMG_FOLDERS_SLICE, train_images),\n",
    "        (VALIDATION_IMG_SRC_FOLDER, VALIDATION_IMG_FOLDERS_SLICE, validation_images)\n",
    "    ]\n",
    "    for (base, folder, dic) in df_config:\n",
    "        for img_folder, img_label in folder.items():\n",
    "            search_folder = \"{}/{}\".format(base, img_folder)\n",
    "            imgs_filename = sorted(get_file_path(search_folder, search_filter = cur_subfile))\n",
    "            dic[\"id\"].extend(imgs_filename)\n",
    "            dic[\"label\"].extend([img_label] * len(imgs_filename))\n",
    "\n",
    "        dfs.append(pd.DataFrame(data=dic))\n",
    "#     print(dfs)\n",
    "    train_df, validation_df = dfs[0], dfs[1]\n",
    "\n",
    "\n",
    "    if not os.path.exists(\"logs/\"): \n",
    "        os.mkdir(\"logs/\")\n",
    "    if not os.path.exists(LOG_FOLDER): \n",
    "        os.mkdir(LOG_FOLDER)\n",
    "        \n",
    "    train_df.to_csv(\"{}/train{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "    validation_df.to_csv(\"{}/test{}.csv\".format(LOG_FOLDER, fold_number), index=False)\n",
    "\n",
    "    print(\"Train fold with {} images\".format(len(train_df)))\n",
    "    print(train_df.groupby(\"label\").label.count())\n",
    "    print()\n",
    "    print(\"Validation fold with {} images\".format(len(validation_df)))\n",
    "    print(validation_df.groupby(\"label\").label.count())\n",
    "    print(\"-\" * 30)\n",
    "    return (train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(dataframe, x_col, y_col, subset=None, shuffle=True, batch_size=32, class_mode=\"binary\"):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=False,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    )\n",
    "    \n",
    "    data_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        subset=subset,\n",
    "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "        class_mode=class_mode,\n",
    "        # color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    base_model = tf.keras.applications.ResNet101(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_resnet101():\n",
    "    with tf.device('/GPU:0'):\n",
    "        conv_base = get_base_model()\n",
    "        conv_base.trainable = True\n",
    "        \n",
    "        x = conv_base.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        '''Added'''\n",
    "        x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        preds = tf.keras.layers.Dense(units=NUM_CLASSES, activation = 'softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=conv_base.input, outputs=preds)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5),\n",
    "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return (model, 'resnet101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_df, validation_df, epochs, fold, axis):\n",
    "    batch_size = 8\n",
    "    train_generator = get_data_generator(train_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "#     validation_generator = get_data_generator(validation_df, \"id\", \"label\", batch_size=batch_size, class_mode=\"categorical\")\n",
    "\n",
    "    print(train_generator.class_indices)\n",
    "#     print(validation_generator.class_indices)\n",
    "    \n",
    "    step_size_train = train_generator.n // train_generator.batch_size\n",
    "#     step_size_validation = validation_generator.n // validation_generator.batch_size\n",
    "\n",
    "    if step_size_train == 0:\n",
    "        step_size_train = train_generator.n // 2\n",
    "#         step_size_validation = validation_generator.n // 2\n",
    "        \n",
    "        \n",
    "    # callbacks, save each time\n",
    "    # training/20200827/vgg16/fold4/axis2\n",
    "    checkpoint_path = \"training/\"\n",
    "    if not os.path.exists(\"training/\"): \n",
    "        os.mkdir(\"training/\")\n",
    "    checkpoint_path = \"{}/\".format(TRAINING_FOLDER)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    checkpoint_path = \"{}/{}/fold{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "    \n",
    "    checkpoint_path = \"{}/{}/fold{}/{}/\".format(TRAINING_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(checkpoint_path): \n",
    "        os.mkdir(checkpoint_path)\n",
    "        \n",
    "    # Save dict results of history and legend from current model\n",
    "    # models/20200827/vgg16/fold4/axis2/{history|legend}\n",
    "    if not os.path.exists(\"models/\"): \n",
    "        os.mkdir(\"models/\")\n",
    "    \n",
    "    model_dir = \"{}/\".format(MODEL_FOLDER)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}\".format(MODEL_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    model_dir = \"{}/{}/fold{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_dir = \"{}/{}/fold{}/{}/\".format(MODEL_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(model_dir): \n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "#     checkpoint_path = checkpoint_path + \"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_path = checkpoint_path +\"/my_checkpoint-{epoch:04d}\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every 25 epochs\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=1,\n",
    "        save_freq=2*batch_size\n",
    "#         monitor='val_accuracy',\n",
    "#         mode='max',\n",
    "#         save_best_only=True\n",
    "    )\n",
    "        \n",
    "    \n",
    "    class_weight = {0: 96/308, 1: 212/308}\n",
    "    history = model.fit(train_generator, # X_Train\n",
    "        steps_per_epoch=step_size_train,\n",
    "        epochs=epochs, \n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=step_size_validation,\n",
    "        callbacks=cp_callback,\n",
    "        class_weight=class_weight\n",
    "                       )\n",
    "    \n",
    "    # Save last values\n",
    "#     model.save_weights(checkpoint_dir+\"/my_checkpoint\")\n",
    "    # model.save(checkpoint_dir+\"/my_checkpoint\")\n",
    "    \n",
    "    # Save history\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = model_dir + 'history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    \n",
    "    # Save classes\n",
    "    print(train_generator.class_indices)\n",
    "    np.save(model_dir + 'legend', train_generator.class_indices)\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, sub_folder, fold, sel_model):\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    \n",
    "    image_dir = \"images/\"\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/\".format(IMAGE_FOLDER)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    image_dir = \"{}/{}/fold{}/{}/\".format(IMAGE_FOLDER, SELECTED_MODEL, fold, axis)\n",
    "    if not os.path.exists(image_dir): \n",
    "        os.mkdir(image_dir)\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_dir + 'accuracy', pad_inches=0.1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(image_dir + 'loss', orientation='portrait', pad_inches=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate model for data folders (patients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_by_patient(model, patients, legend_file, axis):\n",
    "    results = []\n",
    "    for p in patients:\n",
    "        # for axis in SUB_FILE:\n",
    "        curr_dir = \"{}/{}/{}\".format(VALIDATION_IMG_SRC_FOLDER, p, axis)\n",
    "        imgs_filename = sorted(os.listdir(curr_dir))\n",
    "        test_filenames = imgs_filename[:]\n",
    "        test_df = pd.DataFrame({\n",
    "                'filename': test_filenames\n",
    "            })\n",
    "        nb_samples = test_df.shape[0]\n",
    "\n",
    "        test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_gen.flow_from_dataframe(\n",
    "                test_df, \n",
    "                curr_dir, \n",
    "                x_col='filename',\n",
    "                y_col=None,\n",
    "                class_mode=None,\n",
    "                target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                batch_size=16,\n",
    "                shuffle=False\n",
    "        )\n",
    "\n",
    "        predict = model.predict(test_generator, steps=np.ceil(nb_samples/16))\n",
    "\n",
    "        test_df['predicted'] = [np.where(pr == np.max(pr))[0][0] for pr in predict]\n",
    "        test_df['patient'] = p\n",
    "        test_df['axis'] = axis\n",
    "        results.append(test_df)\n",
    "\n",
    "    print('Axis: ', test_df['axis'][0])\n",
    "    df_result = pd.DataFrame(columns=['predicted', 'patient','count'])\n",
    "    for i,test_df in enumerate(results):\n",
    "        cur_patient = test_df['patient'][0]\n",
    "        if os.path.isfile(legend_file+'.npy'):\n",
    "            class_indices = np.load(legend_file+'.npy', allow_pickle=True).item()\n",
    "            class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "            test_df['predicted'] = test_df['predicted'].replace(class_indices)\n",
    "        test_df['count'] = 1\n",
    "        test_df = test_df.groupby('predicted', as_index = False)['count'].count()\n",
    "        test_df['patient'] = cur_patient\n",
    "        df_result = df_result.append(test_df)\n",
    "    return df_result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model for 'axis1...n' and folds [ 1...N ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    \n",
    "    ''' CREATE PATHS FOR SAVE OUTPUT '''\n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    ''''''\n",
    "    \n",
    "    ''' CREATE DATAFRAME AND CONFUSION MATRIX - ACCUMULATED '''\n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "    ''''''\n",
    "    \n",
    "    for n_fold in [1]:\n",
    "#     for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_train = pd.read_csv(\"{}/train/train{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        train_df, validation_df = get_data_set(n_fold, axis, data_train, data_validation)\n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "        print('\\n'+axis+'\\n=====')\n",
    "        model, SELECTED_MODEL = get_model_resnet101()\n",
    "        history = train_model(model, train_df, None, EPOCHS, n_fold, axis)\n",
    "        \n",
    "        #Plot Results\n",
    "        plot_results(history, axis, n_fold, SELECTED_MODEL)\n",
    "        # Load legend\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check models!\n",
    "Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HMV + HCPA\n",
    "'''\n",
    "data_train = pd.read_csv(\"{}/test/test{}_binario.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=======================================================================================')\n",
    "    \n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    \n",
    "    \n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "\n",
    "    for n_fold in [1]: \n",
    "#     for n_fold in [j+1 for j in range(5)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        #data_validation = pd.read_csv(\"{}/test/test{}_binario.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        data_validation = pd.read_csv(\"{}/all_classes_hmv_hcpa_TEST_binario.csv\".format(STRUCTURE_DATASET_FOLDER, n_fold))\n",
    "        \n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "        model, SELECTED_MODEL = get_model_resnet101()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        model = tf.keras.models.load_model(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "#             print(row)\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        del model\n",
    "        \n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted.csv', index=False)\n",
    "    print(labels_pos_dict)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    '''\n",
    "    PRINT METRICS AND CONFUSION MATRIX\n",
    "    '''\n",
    "    pm.plot_labels_metrics(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title='Metrics - ' + axis,\n",
    "                clear_diagonal=False,\n",
    "                figsize=(15, 105),\n",
    "                output_file=\"metrics.png\"\n",
    "            )\n",
    "    pm.plot_confusion_matrix(\n",
    "                cm=confusion_matrix,\n",
    "                normalize=False,\n",
    "                labels=labels,\n",
    "                show_zero=False,\n",
    "                title=\"Confusion Matrix - \" + axis,\n",
    "                clear_diagonal=False,\n",
    "                output_file=\"matrix.png\",\n",
    "                figsize=(10, 7)\n",
    "#                 verbose=args.verbose,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pos_dict\n",
    "labels = data_train['covid'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' #########################################################\n",
    "TEST 6\n",
    "''' #######################################################\n",
    "\n",
    "data_train = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 1))\n",
    "labels = data_train['covid'].unique()\n",
    "\n",
    "labels.sort()\n",
    "labels_length = len(labels)\n",
    "labels_pos_dict = dict(zip(labels, [i for i in range(labels_length)]))\n",
    "\n",
    "\n",
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=======================================================================================')\n",
    "    \n",
    "    predicted_dir = \"{}/\".format(OUTPUT_PREDICTED_FOLDER)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    predicted_dir = \"{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    \n",
    "    \n",
    "    # Predicted class for patient and fold\n",
    "    df_axis = pd.DataFrame(columns=['fold', 'patient', 'real', 'predicted', 'count'])\n",
    "    \n",
    "#     for n_fold in [1, 2, 3, 4, 5]: \n",
    "    for n_fold in [j+1 for j in range(5)]:\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        confusion_matrix = np.zeros(labels_length*labels_length).reshape(labels_length, labels_length)\n",
    "\n",
    "        \n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))\n",
    "        \n",
    "        data_validation = pd.read_csv(\"{}/test/test{}.csv\".format(STRUCTURE_DATASET_FOLDER, 7))\n",
    "        \n",
    "        data_validation_dict = dict(zip(data_validation.nome, data_validation.covid))\n",
    "        \n",
    "        model, SELECTED_MODEL = get_model_resnet101()\n",
    "        legend_path = \"{}/{}/fold{}/{}/legend\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis)\n",
    "        \n",
    "        model = tf.keras.models.load_model(\"{}/{}/fold{}/{}/my_checkpoint\".format(TRAINING_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        \n",
    "        # Test with other patients\n",
    "        df = predictions_by_patient(model, data_validation['nome'].to_list(), legend_path, axis)\n",
    "        idx = df.groupby(['patient'])['count'].transform(max) == df['count']\n",
    "        df = df[idx]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df['fold'] = n_fold\n",
    "        df['real'] = ''\n",
    "        last_patient = ''\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if row['patient'] != last_patient:\n",
    "                df.loc[index, 'real'] = data_validation_dict[row['patient']]\n",
    "                real_label_pos = labels_pos_dict[data_validation_dict[row['patient']]]\n",
    "                pred_label_pos = labels_pos_dict[row['predicted']]\n",
    "                confusion_matrix[real_label_pos][pred_label_pos] += 1\n",
    "            last_patient = row['patient']        \n",
    "        df_axis = df_axis.append(df)\n",
    "        del model\n",
    "        \n",
    "        print(labels_pos_dict)\n",
    "        print(confusion_matrix)\n",
    "\n",
    "        '''\n",
    "        PRINT METRICS AND CONFUSION MATRIX\n",
    "        '''\n",
    "        pm.plot_labels_metrics(\n",
    "                    cm=confusion_matrix,\n",
    "                    normalize=False,\n",
    "                    labels=labels,\n",
    "                    show_zero=False,\n",
    "                    title='Metrics - ' + axis,\n",
    "                    clear_diagonal=False,\n",
    "                    figsize=(15, 105),\n",
    "                    output_file=\"metrics.png\"\n",
    "                )\n",
    "        pm.plot_confusion_matrix(\n",
    "                    cm=confusion_matrix,\n",
    "                    normalize=False,\n",
    "                    labels=labels,\n",
    "                    show_zero=False,\n",
    "                    title=\"Confusion Matrix - \" + axis,\n",
    "                    clear_diagonal=False,\n",
    "                    output_file=\"matrix.png\",\n",
    "                    figsize=(10, 7)\n",
    "                )\n",
    "    predicted_dir = \"{}/{}/{}/\".format(OUTPUT_PREDICTED_FOLDER, axis, SELECTED_MODEL)\n",
    "    if not os.path.exists(predicted_dir): \n",
    "        os.mkdir(predicted_dir)\n",
    "    df_axis.to_csv(predicted_dir+'predicted_test7_hcpa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axis in SUB_FILE:\n",
    "    print('\\n'+axis+'\\n=====')\n",
    "    for n_fold in [j+1 for j in range(4)]:\n",
    "        print(\"\\n\\n\\nFold\", str(n_fold))        \n",
    "        #model, SELECTED_MODEL = get_model_resnet50()\n",
    "        SELECTED_MODEL = 'resnet101'\n",
    "        df = pd.read_csv(\"{}/{}/fold{}/{}/history.csv\".format(MODEL_FOLDER, SELECTED_MODEL, n_fold, axis))\n",
    "        print(max(df['val_accuracy']))\n",
    "        print((df['val_accuracy'].mean()))\n",
    "#         print(df['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
